---
title: "R Notebook"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: console
---

```{r include=FALSE}
knitr::opts_chunk$set(comment = '>')
```

### CS5706
### Lab 08 - Prediction methods in R
#### Tutorial
*Alessandro Pandini*  
Revision: 1.01

***
#### Table of contents

1. [Install kernlab and class packages](#Install_packages)
2. [SVM training (binary classifier)](#SVM_training_binary_classifier)
3. [SVM prediction (binary classifier)](#SVM_prediction_binary_classifier)
4. [Data preparation](#Data_preparation)
5. [SVM training](#SVM_training)
6. [SVM prediction](#SVM_prediction)
7. [k-nearest neighbour prediction](#k-nn_prediction)

***

#### 1. Install kernlab and class packages{#Install_packages}
```{r}
# install the kernlab (for SVM) and class (for kNN) packages from CRAN
if(require(kernlab) == FALSE){
  install.packages('kernlab')
  library(kernlab)
}
if(require(class) == FALSE){
  install.packages('class')
  library(class)
}
```

#### 2. SVM training (binary classifier){#SVM_training_binary_classifier}
```{r}
# load and inspect the training/test datasets
#   note: this is a synthetic dataset of (x, y) points
training_syndata <- read.csv("syndata_training.csv")
test_syndata <- read.csv("syndata_test.csv")
str(training_syndata)
str(test_syndata)
# define a formula for predicting the class variable
syndata_formula = class ~ x + y

# train an SVM model with a linear kernel (vanilladot) and cost set to 1
lk_svm_syndata <- ksvm(syndata_formula, data = training_syndata, kernel = 'vanilladot', C = 1, type = "C-svc")

# plot the SVM object for this binary classifier
#   note: this plot is only available for binary classifiers in 2D
#     the plot includes decision values that in 2D can be interpreted
#     as the distances from the hyperplane
plot(lk_svm_syndata, data = training_syndata)
```




#### 3. SVM prediction (binary classifier){#SVM_prediction_binary_classifier}
```{r}
# compute the prediction with the SVM model
#   note: the class attribute (column 1) is excluded from the test data set
lk_svm_syndata_pred <- predict(lk_svm_syndata, test_syndata[,-1])

# create a contingency table for the actual VS predicted for the SVM model
lk_svm_results_table <- table(svm = lk_svm_syndata_pred,  actual = test_syndata$class)
lk_svm_results_table

# calculate the accuracy from the contingency table
acc_lk_svm <- sum(diag(lk_svm_results_table)) / sum(lk_svm_results_table)
acc_lk_svm
```

#### 4. Data preparation{#Data_preparation}
```{r}
# read the data from the letters_data.csv file
#   note: the dataset is derived from the letter-recognition dataset
#     documentation on the original dataset is available here:
#     https://archive.ics.uci.edu/ml/datasets/letter+recognition
letters_data <- read.csv("letters_data.csv", stringsAsFactors = T)

# inspect the data
str(letters_data)

# set random seed
set.seed(1999)
# create a 70/30 training/test set split
n_rows <- nrow(letters_data)
# sample 70% (n_rows * 0.7) indices in the ranges 1:nrows
training_idx <- sample(n_rows, n_rows * 0.7)
# filter the data frame with the training indices (and the complement)
training_letters_data <- letters_data[training_idx,]
test_letters_data <- letters_data[-training_idx,]
```

#### 5. SVM training{#SVM_training}
```{r}
# define a formula for predicting lettr
#   note: the reformulate function simplifies the definition of the formula
#     and makes the code more general
letters_data_formula <- reformulate(names(training_letters_data[, -1]), response = 'lettr')

# train two SVM models with different kernel functions
#   note: ksvm automatically scales the attribute values
#     there is no need to perform a preliminary data transformation
# train an SVM model with a linear kernel (vanilladot) and cost set to 1
lk_svm_letters_data <- ksvm(letters_data_formula, data = training_letters_data, kernel = 'vanilladot', C = 1)
# train an SVM model with a radial basis kernel (rfbdot) and cost set to 1
rbf_svm_letters_data <- ksvm(letters_data_formula, data = training_letters_data, kernel = 'rbfdot', C = 1)

#plots
#plot(lk_svm_letters_data, data = training_letters_data)

```

#### 6. SVM prediction}{#SVM_prediction}
```{r}
# compute the prediction with both SVM models
#   note: the lettr attribute (column 1) is excluded from the test data set
lk_svm_letters_data_pred <- predict(lk_svm_letters_data, test_letters_data[,-1])
rbf_svm_letters_data_pred <- predict(rbf_svm_letters_data, test_letters_data[,-1])

# create a contingency table for the actual VS predicted for both SVM models
lk_svm_results_table <- table(svm = lk_svm_letters_data_pred,  actual = test_letters_data$lettr)
lk_svm_results_table
rbf_svm_results_table <- table(svm = rbf_svm_letters_data_pred,  actual = test_letters_data$lettr)
rbf_svm_results_table

# calculate the accuracy from each contingency table
#   as sum of diagonal elements over sum of the matrix values
acc_lk_svm <- sum(diag(lk_svm_results_table)) / sum(lk_svm_results_table)
acc_lk_svm
acc_rbf_svm <- sum(diag(rbf_svm_results_table)) / sum(rbf_svm_results_table)
acc_rbf_svm
```

#### 7. k-nearest neighbour prediction{#k-nn_prediction}
```{r}
# transform the data using a min-max function
# first define a MinMax function
MinMax <- function(x){
  tx <- (x - min(x)) / (max(x) - min(x))
  return(tx)
}
# then apply the function to each column of the data set
#   note: the lettr attribute should be excluded
letters_data_minmax <- apply(letters_data[,-1], 2, MinMax)
# the matrix needs to be 'cast' into a data frame
letters_data_minmax <- as.data.frame(letters_data_minmax)
# finally add back the lettr attribute
letters_data_minmax <- cbind(letters_data$lettr, letters_data_minmax)

# filter the data frame with the (previously generated) training indices
training_letters_data_minmax <- letters_data_minmax[training_idx,]
test_letters_data_minmax <- letters_data_minmax[-training_idx,]

# predict the class for the test dataset
#   note: k is set to the square root of the number of instances in the training set
#     the syntax of the knn command is different from the one for other ML methods
#     you can find more information in the help of knn
k_value = sqrt(dim(training_letters_data_minmax)[1])
knn_letters_data_minmax_pred <- knn(train = training_letters_data_minmax[,-1], test = test_letters_data_minmax[,-1], cl = training_letters_data_minmax[,1], k = k_value)

# create a table with actual values and the predictions
letters_data_minmax_results <- data.frame(
  actual = test_letters_data_minmax[,1],
  knn = knn_letters_data_minmax_pred
)

# create a contingency table for the actual VS predicted
knn_results_table <- table(letters_data_minmax_results[,c('actual', 'knn')])
knn_results_table

# calculate accuracy from the contingency table
acc_knn <- sum(diag(knn_results_table)) / sum(knn_results_table)
acc_knn
```

